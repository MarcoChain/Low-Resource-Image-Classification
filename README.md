# Low-Resource-Image-Classification

A non-parametric method with 14 lines of codes and no training time beats a non fine-tuned BERT on out-of-distribution (OOD) datasets. ðŸ¤¯


While on one hand, OpenAI introduces GPT-4, surpassing one trillion parameters, on the other hand, this highly intriguing approach (https://lnkd.in/eUyETRjq) combines a simple compressor like gZip ðŸ—œ with a k-nearest-neighbour classifier (KNN) and proves to be proficient across various tasks, from text classification to few-shot learning. 
The compressor was chosen from various lossless compressors, including: bz2, lzma, and zstandard. Having said that, gZip appears to strike the perfect balance between final performance and compression time.
Since the output generated by a compressor is a sequence of bytes, instead of using the typical Euclidean distance for KNN, they employed an approximation of Kolmogorov complexity called Normalised Compression Distance (NCD). Thus, for two documents x and y, C(x) represents the length of x after compression using gzip, while C(xy) is the compressed length of their concatenation.
Based on this notation, NCD = [ C(xy) - min( c(x), c(y) ) ]/max( c(x), c(y) ). So, if x and y are two highly similar documents, then c(xy) will be close to c(x) and, in turn, close to c(y). This implies that the numerator will be a very small value, and the NCD will be very close to zero. On the other hand, if x and y have little in common, c(xy) will be approximately equal to c(x) + c(y), and thus the NCD will be close to 1.

This algorithm, though very simple, achieves outstanding results on textual documents. As we have mentioned before, it manages to challenge a well-established architecture like BERT in various OOD tasks.
Intrigued by these results and the approach, I attempted to apply the same algorithm for image classification, starting with the MNIST dataset. After converting the images into vectors, I serialised them using pickle, compressed them with gZip, and then applied KNN combined with NCD. Unfortunately, the results were not as promising. In fact, I could not surpass 40% accuracy and the inference time was incredibly high. Evidently, gzip is not designed to preserve spatial information like it does for textual data, and the decline in accuracy is a natural consequence.
